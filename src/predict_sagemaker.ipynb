{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import hashlib\n",
    "import os \n",
    "from utils import logger\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from utils import logger\n",
    "#def lassoSelection(X,y,)\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import time\n",
    "\n",
    "def scores_for_ks(test_labels, knn_labels, ks):\n",
    "    #f1_weight = []\n",
    "    #f1_macro = []\n",
    "    #f1_micro = []\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    for k in ks:\n",
    "        pred_k = stats.mode(knn_labels[:,:k], axis=1)[0].reshape((-1,))\n",
    "        print(pred_k)\n",
    "        #f1_weight.append(f1_score(test_labels, pred_k, average='weighted'))\n",
    "        #f1_macro.append(f1_score(test_labels, pred_k, average='macro'))\n",
    "        f1.append(f1_score(test_labels, pred_k))\n",
    "        acc.append(accuracy_score(test_labels, pred_k))\n",
    "    return {'f1': f1, 'accuracy': acc, }\n",
    "\n",
    "def plot_prediction_quality(scores, ks):\n",
    "    colors = ['r-', 'b-', 'g-','y-'][:len(scores)]\n",
    "    for (k,v), color in zip(scores.items(), colors):\n",
    "        plt.plot(ks, v, color, label=k)\n",
    "    plt.legend()\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('prediction quality')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_quality(predictor, test_features, test_labels, model_name, verbose=True, num_batches=1):\n",
    "    \"\"\"\n",
    "    Evaluate quality metrics of a model on a test set. \n",
    "    \"\"\"\n",
    "    # tune the predictor to provide the verbose response\n",
    "    predictor.accept = 'application/json; verbose=true'\n",
    "    \n",
    "    # split the test data set into num_batches batches and evaluate using prediction endpoint. \n",
    "    print('running prediction (quality)...')\n",
    "    batches = np.array_split(test_features, num_batches)\n",
    "    knn_labels = []\n",
    "    for batch in batches:\n",
    "        pred_result = predictor.predict(batch)\n",
    "        cur_knn_labels = np.array([pred_result['predictions'][i]['labels'] for i in range(len(pred_result['predictions']))])\n",
    "        knn_labels.append(cur_knn_labels)\n",
    "    knn_labels = np.concatenate(knn_labels)\n",
    "    print('running prediction (quality)... done')\n",
    "    print(knn_labels)\n",
    "    print(test_labels)\n",
    "    # figure out different k values\n",
    "    top_k = knn_labels.shape[1]\n",
    "    ks = range(1, top_k+1)\n",
    "    \n",
    "    # compute scores for the quality of the model for each value of k\n",
    "    print('computing scores for all values of k... ')\n",
    "    quality_scores = scores_for_ks(test_labels, knn_labels, ks)\n",
    "    print('computing scores for all values of k... done')\n",
    "    if verbose:\n",
    "        plot_prediction_quality(quality_scores, ks)\n",
    "    \n",
    "    return quality_scores\n",
    "\n",
    "def evaluate_latency(predictor, test_features, test_labels, model_name, verbose=True, num_batches=1):\n",
    "    \"\"\"\n",
    "    Evaluate the run-time of a model on a test set.\n",
    "    \"\"\"\n",
    "    # tune the predictor to provide the non-verbose response\n",
    "    predictor.accept = 'application/json'\n",
    "    \n",
    "    # latency for large batches:\n",
    "    # split the test data set into num_batches batches and evaluate the latencies of the calls to endpoint. \n",
    "    print('running prediction (latency)...')\n",
    "    batches = np.array_split(test_features, num_batches)\n",
    "    test_preds = []\n",
    "    latency_sum = 0\n",
    "    for batch in batches:\n",
    "        start = time.time()\n",
    "        pred_batch = predictor.predict(batch)\n",
    "        latency_sum += time.time() - start\n",
    "    latency_mean = latency_sum / float(num_batches)\n",
    "    avg_batch_size = test_features.shape[0] / num_batches\n",
    "    \n",
    "    # estimate the latency for a batch of size 1\n",
    "    latencies = []\n",
    "    attempts = 128\n",
    "    for i in range(attempts):\n",
    "        start = time.time()\n",
    "        pred_batch = predictor.predict(test_features[i].reshape((1,-1)))\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    latencies = sorted(latencies)\n",
    "    latency1_mean = sum(latencies) / float(attempts)\n",
    "    latency1_p90 = latencies[int(attempts*0.9)]\n",
    "    latency1_p99 = latencies[int(attempts*0.99)]\n",
    "    print('running prediction (latency)... done')\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"{:<11} {:.3f}\".format('Latency (ms, batch size %d):' % avg_batch_size, latency_mean * 1000))\n",
    "        print(\"{:<11} {:.3f}\".format('Latency (ms) mean for single item:', latency1_mean * 1000))\n",
    "        print(\"{:<11} {:.3f}\".format('Latency (ms) p90 for single item:', latency1_p90 * 1000))\n",
    "        print(\"{:<11} {:.3f}\".format('Latency (ms) p99 for single item:', latency1_p99 * 1000))\n",
    "        \n",
    "    return {'Latency': latency_mean, 'Latency1_mean': latency1_mean, 'Latency1_p90': latency1_p90, \n",
    "            'Latency1_p99': latency1_p99}\n",
    "\n",
    "def evaluate(predictor, test_features, test_labels, model_name, verbose=True, num_batches=100):\n",
    "    eval_result_q = evaluate_quality(pred, test_features, test_labels, model_name=model_name, verbose=verbose, num_batches=num_batches)\n",
    "    eval_result_l = evaluate_latency(pred, test_features, test_labels, model_name=model_name, verbose=verbose, num_batches=num_batches)\n",
    "    return dict(list(eval_result_q.items()) + list(eval_result_l.items()))\n",
    "\n",
    "def lassoSelection(X_train, y_train, n):\n",
    "    '''\n",
    "    Lasso feature selection.  Select n features. \n",
    "    '''\n",
    "    #lasso feature selection\n",
    "    #print (X_train)\n",
    "    clf = LassoCV()\n",
    "    sfm = SelectFromModel(clf, threshold=0)\n",
    "    sfm.fit(X_train, y_train)\n",
    "    X_transform = sfm.transform(X_train)\n",
    "    n_features = X_transform.shape[1]\n",
    "    \n",
    "    #print(n_features)\n",
    "    while n_features > n:\n",
    "        sfm.threshold += 0.01\n",
    "        X_transform = sfm.transform(X_train)\n",
    "        n_features = X_transform.shape[1]\n",
    "    features = [index for index,value in enumerate(sfm.get_support()) if value == True  ]\n",
    "    logger.info(\"selected features are {}\".format(features))\n",
    "    return features\n",
    "\n",
    "\n",
    "def specificity_score(y_true, y_predict):\n",
    "    '''\n",
    "    true_negative rate\n",
    "    '''\n",
    "    true_negative = len([index for index,pair in enumerate(zip(y_true,y_predict)) if pair[0]==pair[1] and pair[0]==0 ])\n",
    "    real_negative = len(y_true) - sum(y_true)\n",
    "    return true_negative / real_negative \n",
    "\n",
    "def delete_endpoint(predictor):\n",
    "    try:\n",
    "        boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "        print('Deleted {}'.format(predictor.endpoint))\n",
    "    except:\n",
    "        print('Already deleted: {}'.format(predictor.endpoint))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # specify algorithm containers. These contain the code for the training job\n",
    "    containers = {\n",
    "        'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/knn:1',\n",
    "        'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/knn:1',\n",
    "        'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/knn:1',\n",
    "        'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/knn:1',\n",
    "        'ap-northeast-1': '351501993468.dkr.ecr.ap-northeast-1.amazonaws.com/knn:1',\n",
    "        'ap-northeast-2': '835164637446.dkr.ecr.ap-northeast-2.amazonaws.com/knn:1',\n",
    "        'ap-southeast-2': '712309505854.dkr.ecr.ap-southeast-2.amazonaws.com/knn:1'\n",
    "    }\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m5.2xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    if s3_test_data is not None:\n",
    "        fit_input['test'] = s3_test_data\n",
    "    knn.fit(fit_input)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictor_from_hyperparams(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                                        endpoint_name=endpoint_name)\n",
    "    knn_predictor.content_type = 'text/csv'\n",
    "    knn_predictor.serializer = csv_serializer\n",
    "    knn_predictor.deserializer = json_deserializer\n",
    "    return knn_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highlight_apx_max(row):\n",
    "    '''\n",
    "    highlight the aproximate best (max or min) in a Series yellow.\n",
    "    '''\n",
    "    max_val = row.max()\n",
    "    colors = ['background-color: yellow' if cur_val >= max_val * 0.9975 else '' for cur_val in row]\n",
    "        \n",
    "    return colors\n",
    "def highlight_far_from_min(row):\n",
    "    '''\n",
    "    highlight the aproximate best (max or min) in a Series yellow.\n",
    "    '''\n",
    "    med_val = row.median()\n",
    "    colors = ['background-color: red' if cur_val >= med_val * 1.2 else '' for cur_val in row]\n",
    "        \n",
    "    return colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "[2018-10-20 13:47:19,953 - GDC - INFO] Percentage of tumor cases in training set is 0\n",
      "[2018-10-20 13:47:19,954 - GDC - INFO] Percentage of tumor cases in test set is 0\n",
      "/Users/xin/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[2018-10-20 13:47:21,954 - GDC - INFO] selected features are [88, 185, 505, 969, 1099]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named sagemaker.amazon.common",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0391e48af320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamazon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msmac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sagemaker.amazon.common"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    data_dir =\"../data/\"\n",
    "\n",
    "    data_file = data_dir + \"miRNA_matrix.csv\"\n",
    "\n",
    "    df = pd.read_csv(data_file)\n",
    "    # print(df)\n",
    "    y_data = df.pop('label').values\n",
    "\n",
    "    df.pop('file_id')\n",
    "\n",
    "    columns =df.columns\n",
    "    #print (columns)\n",
    "    X_data = df.values\n",
    "\n",
    "    # split the data to train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=0)\n",
    "    \n",
    "\n",
    "    #standardize the data.\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # check the distribution of tumor and normal sampels in traing and test data set.\n",
    "    logger.info(\"Percentage of tumor cases in training set is {}\".format(sum(y_train)/len(y_train)))\n",
    "    logger.info(\"Percentage of tumor cases in test set is {}\".format(sum(y_test)/len(y_test)))\n",
    "    \n",
    "    n = 7\n",
    "    features_columns = lassoSelection(X_train, y_train, n)\n",
    "    \n",
    "    \"\"\"\n",
    "    import io\n",
    "    import sagemaker.amazon.common as smac\n",
    "    import boto3\n",
    "    import sagemaker  \n",
    "    from sagemaker import get_execution_role\n",
    "    from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "    bucket =  \"cancer-bucket\"\n",
    "    prefix = 'knn-prediction'\n",
    "    key = 'cancer-data'\n",
    "    \n",
    "    #write the train data to S3\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(buf, X_train[:,features_columns], y_train)\n",
    "    buf.seek(0)   \n",
    "    boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "    s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)    \n",
    "    print('uploaded training data location: {}'.format(s3_train_data)) \n",
    "    \"\"\"\n",
    "    print(X_train[:,features_columns].shape)\n",
    "\"\"\"\n",
    "    #write the test data to S3\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(buf, X_test[:,features_columns], y_test)\n",
    "    buf.seek(0)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test', key)).upload_fileobj(buf)\n",
    "    s3_test_data = 's3://{}/{}/test/{}'.format(bucket, prefix, key)\n",
    "    print('uploaded test data location: {}'.format(s3_test_data))    \n",
    "\"\"\"\n",
    "    print(X_test[:,features_columns].shape)\n",
    "    \n",
    "    #scores = model_fit_predict(X_train[:,feaures_columns],X_test[:,feaures_columns],y_train,y_test)\n",
    "\n",
    "    #draw(scores)\n",
    "    #lasso cross validation\n",
    "    # lassoreg = Lasso(random_state=0)\n",
    "    # alphas = np.logspace(-4, -0.5, 30)\n",
    "    # tuned_parameters = [{'alpha': alphas}]\n",
    "    # n_fold = 10\n",
    "    # clf = GridSearchCV(lassoreg,tuned_parameters,cv=10, refit = False)\n",
    "    # clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hyperparams_flat_l2 = {\n",
    "    'feature_dim': 5,\n",
    "    'k': 100,\n",
    "    'sample_size': 297,\n",
    "    'predictor_type': 'classifier' \n",
    "    # NOTE: The default distance is L2 and index is Flat, so we don't list them here\n",
    "}\n",
    "output_path_flat_l2 = 's3://' + bucket + '/' + prefix + '/flat_l2/output'\n",
    "knn_estimator_flat_l2 = trained_estimator_from_hyperparams(s3_train_data, hyperparams_flat_l2, output_path_flat_l2, \n",
    "                                                           s3_test_data=s3_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "instance_types = ['ml.m4.xlarge']\n",
    "index2estimator = {'flat_l2': knn_estimator_flat_l2}\n",
    "\n",
    "eval_results = {}\n",
    "\n",
    "for index in index2estimator:\n",
    "    estimator = index2estimator[index]\n",
    "    eval_results[index] = {}\n",
    "    for instance_type in instance_types:\n",
    "        model_name = 'knn_%s_%s'%(index, instance_type)\n",
    "        endpoint_name = 'knn-latency-%s-%s-%s'%(index.replace('_','-'), instance_type.replace('.','-'),\n",
    "                                               str(time.time()).replace('.','-'))\n",
    "        print('\\nsetting up endpoint for instance_type=%s, index_type=%s' %(instance_type, index))\n",
    "        pred = predictor_from_hyperparams(estimator, index, instance_type, endpoint_name=endpoint_name)\n",
    "        print('')\n",
    "        eval_result = evaluate(pred,X_test[:,features_columns], y_test, model_name=model_name, verbose=True)        \n",
    "        eval_result['instance'] = instance_type \n",
    "        eval_result['index'] = index \n",
    "        eval_results[index][instance_type] = eval_result\n",
    "        delete_endpoint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "k_range = range(1, 13)\n",
    "df_index = []\n",
    "data = []\n",
    "columns_lat = ['latency_mean', 'latency1_mean', 'latency1_p90', 'latency1_p99']\n",
    "columns_acc = ['acc_%d' % k for k in k_range]\n",
    "columns = columns_lat + columns_acc\n",
    "#print(eval_result)\n",
    "print (eval_results)\n",
    "for index, index_res in eval_results.items():\n",
    "    print (index)\n",
    "    print (index_res)\n",
    "    for instance, res in index_res.items():\n",
    "        # for sample size?\n",
    "        print(instance)\n",
    "        print(res)\n",
    "        df_index.append(index+'_'+instance)\n",
    "        latencies = np.array([res['Latency'], res['Latency1_mean'], res['Latency1_p90'], res['Latency1_p99']])\n",
    "        row = np.concatenate([latencies*10,\n",
    "                             res['accuracy'][k_range[0] - 1:k_range[-1] ]])\n",
    "        row *= 100\n",
    "        data.append(row)\n",
    "\n",
    "df = pd.DataFrame(index=df_index, data=data, columns=columns)\n",
    "df_acc = df[columns_acc]\n",
    "df_lat = df[columns_lat]\n",
    "\n",
    "\n",
    "df_acc.round(decimals=1).style.apply(highlight_apx_max, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_lat.round(decimals=1).style.apply(highlight_far_from_min, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
